{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.datasets import URLs, untar_data\n",
    "from pathlib import Path\n",
    "import torch, re, PIL, os, mimetypes, csv, operator, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "import pandas as pd, numpy as np\n",
    "from enum import Enum\n",
    "from torch import tensor,Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data block API from config class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def test(a,b,cmp,cname=None,tst_name=''):\n",
    "    if cname is None: cname=cmp.__name__\n",
    "    assert cmp(a,b),f\"{tst_name},{cname}:\\n{a}\\n{b}\"\n",
    "\n",
    "def test_eq(a,b,tst_name=''): \n",
    "    if isinstance(a, np.ndarray) or (isinstance(a, Tensor) and not len(a.shape) == 0):\n",
    "        assert len(a) == len(b), f\"{tst_name}, lengths mismatch:\\n{a}\\n{b}\"\n",
    "        test(a,b,lambda x,y: (x == y).all(),'==',tst_name)\n",
    "    else: test(a,b,operator.eq,'==',tst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(noop(1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def listify(o):\n",
    "    \"Make `o` a list.\"\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(listify(None), [])\n",
    "test_eq(listify([1,2,3]), [1,2,3])\n",
    "test_eq(listify('abc'), ['abc'])\n",
    "test_eq(listify(range(0,3)), [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    \"Apply all `funcs` to `x` in order, pass along `args` and `kwargs`.\"\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, *args, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# basic behavior\n",
    "def _test_f1(x, a=2): return x**a\n",
    "def _test_f2(x, a=2): return a*x\n",
    "test_eq(compose(2, [_test_f1, _test_f2]), 8)\n",
    "# order\n",
    "_test_f1._order = 1\n",
    "test_eq(compose(2, [_test_f1, _test_f2]), 16)\n",
    "#args\n",
    "test_eq(compose(2, [_test_f1, _test_f2], 3), 216)\n",
    "#kwargs\n",
    "test_eq(compose(2, [_test_f1, _test_f2], a=3), 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def uniqueify(x, sort=False):\n",
    "    \"Return the unqiue elements in `x`, optionally `sort`-ed.\"\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(set(uniqueify([1,1,0,5,0,3])), {0,1,3,5})\n",
    "test_eq(uniqueify([1,1,0,5,0,3], sort=True), [0,1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(setify(None), set())\n",
    "test_eq(setify('abc'), {'abc'})\n",
    "test_eq(setify([1,2,2]), {1,2})\n",
    "test_eq(setify(range(0,3)), {0,1,2})\n",
    "test_eq(setify({1,2}), {1,2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def onehot(x, c):\n",
    "    \"Return the one-hot encoded tensor for `x` with `c` classes.\"\n",
    "    res = torch.zeros(c)\n",
    "    res[x] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(onehot(1,5), tensor([0.,1.,0.,0.,0.]))\n",
    "test_eq(onehot([1,3],5), tensor([0.,1.,0.,1.,0.]))\n",
    "test_eq(onehot(tensor([1,3]),5), tensor([0.,1.,0.,1.,0.]))\n",
    "test_eq(onehot([True,False,True,True,False],5), tensor([1.,0.,1.,1.,0.]))\n",
    "test_eq(onehot([],5), tensor([0.,0.,0.,0.,0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    \"Get all the files in `path` with optional `extensions`.\"\n",
    "    path = Path(path)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "        return res\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        return _get_files(path, f, extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "test_eq(len(get_files(path/'train'/'3')), 346)\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.png')), 346)\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.jpg')), 0)\n",
    "test_eq(len(get_files(path/'train', extensions='.png')), 0)\n",
    "test_eq(len(get_files(path/'train', extensions='.png', recurse=True)), 709)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, include=['train'])), 709)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, include=['train', 'test'])), 729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def grab_idx(batch, i):\n",
    "    \"Return the `i`-th sample in `batch`\"\n",
    "    return [grab_idx(b,i) for b in batch] if isinstance(batch, (list,tuple)) else batch[i].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(grab_idx(tensor([1,2]), 1), 2)\n",
    "test_eq(grab_idx([tensor([1,2]), tensor([3,4])], 1), [2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_column(df, col_name, prefix='', suffix='', delim=None):\n",
    "    \"Read `col_name` in `df`, optionnally adding `prefix` or `suffix`.\"\n",
    "    values = df[col_name].values.astype(str)\n",
    "    values = np.char.add(np.char.add(prefix, values), suffix)\n",
    "    if delim is not None:\n",
    "        values = np.array(list(csv.reader(values, delimiter=delim)))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "df = pd.DataFrame({'a': ['cat', 'dog', 'car'], 'b': ['a b', 'c d', 'a e']})\n",
    "test_eq(read_column(df, 'a'), np.array(['cat', 'dog', 'car']))\n",
    "test_eq(read_column(df, 'a', prefix='o'), np.array(['ocat', 'odog', 'ocar']))\n",
    "test_eq(read_column(df, 'a', suffix='.png'), np.array(['cat.png', 'dog.png', 'car.png']))\n",
    "test_eq(read_column(df, 'b', delim=' '), np.array([['a','b'], ['c','d'], ['a','e']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Transform():\n",
    "    \"A basic class to transform some data.\"\n",
    "    _order=0\n",
    "    def __call__(self, o):  return o\n",
    "    def undo(self, o):      return o\n",
    "    def setup(self, items): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: undo is for the transforms you want to undo when showing data. You want to undo one-hot encoding but not opening an imare or data augmentation transforms for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Item():\n",
    "    \"A basic class for representing some data type.\"\n",
    "    default_tfms = None\n",
    "    default_tfms_xy = None\n",
    "    tfm_kwargs = None\n",
    "    \n",
    "    def init_tfms(self, xy=False):\n",
    "        kwargs = self.tfm_kwargs if self.tfm_kwargs is not None else {}\n",
    "        return [t(**kwargs) for t in listify(self.default_tfms_xy if xy else self.default_tfms)]\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        \"How to show one element `x` on `ax`.\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data block API core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One ItemList to contain them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ItemList():\n",
    "    def __init__(self, items, tfms=None, item_type=Item()): \n",
    "        self.items,self.tfms,self.item_type = items,tfms,item_type\n",
    "        for tfm in listify(self.tfms): tfm.setup(self.items)\n",
    "    def _get(self, i): return compose(i, self.tfms)\n",
    "    def __getitem__(self, idx):\n",
    "        try: return self._get(self.items[idx])\n",
    "        except TypeError:\n",
    "            if isinstance(idx[0],bool):\n",
    "                assert len(idx)==len(self) # bool mask\n",
    "                return [self._get(o) for m,o in zip(idx,self.items) if m]\n",
    "            return [self._get(self.items[i]) for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res\n",
    "    \n",
    "    def deproc(self, o): \n",
    "        \"Reverse transforms on `o`.\"\n",
    "        return compose(o, [t.undo for t in reversed(listify(self.tfms))])\n",
    "    \n",
    "    def show(self, i, ax=None):\n",
    "        \"Show item `i` on `ax`.\"\n",
    "        x = self.deproc(self[i])\n",
    "        if ax is None: _,ax = plt.subplots(1,1)\n",
    "        self.item_type.show(x, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LabeledData():\n",
    "    def __init__(self, x, y, tfms=None): \n",
    "        self.x,self.y,self.tfms = x,y,tfms\n",
    "        for tfm in listify(self.tfms): tfm.setup(self)\n",
    "    def __repr__(self):        return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx): return compose((self.x[idx],self.y[idx]), self.tfms)\n",
    "    def __len__(self):         return len(self.x)\n",
    "    \n",
    "    def deproc(self, o):\n",
    "        \"Reverse transforms on `o`.\"\n",
    "        (x,y) = compose(o, [t.undo for t in reversed(listify(self.tfms))])\n",
    "        return (self.x.deproc(x),self.y.deproc(y))\n",
    "    \n",
    "    def show(self, i, ax=None):\n",
    "        \"Show item and label `i` on `ax`.\"\n",
    "        (x,y) = self.deproc(self[i])\n",
    "        if ax is None: _,ax = plt.subplots(1,1)\n",
    "        self.x.item_type.show(x, ax)\n",
    "        self.y.item_type.show(y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader and DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "def get_dl(ds, bs, shuffle=False, drop_last=False, **kwargs):\n",
    "    \"Basic function to get a `DataLoader`\"\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, drop_last=drop_last, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBunch():\n",
    "    \"Basic wrapper around several `DataLoader`.\"\n",
    "    def __init__(self, train_dl, valid_dl):\n",
    "        self.train_dl,self.valid_dl = train_dl,valid_dl\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class to represent any kind of data. User provides the `get_x_cls` and `get_y_cls` then the four functions. At init everything is constructed with optionals transforms or custom instances of `get_x`/`get_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBlock():\n",
    "    \"Main class to represent a dataset. Subclass the 2 properties and 4 methods below to your need.\"\n",
    "    x_cls = Item #Type of input\n",
    "    y_cls = Item #Type of targer\n",
    "    def get_source(self):         \n",
    "        \"Return the source of your data (path, dataframe...), optionally download it.\"\n",
    "        raise NotImplementedError\n",
    "    def get_items(self, source):  \n",
    "        \"Use `source` to return the list of all items.\"\n",
    "        raise NotImplementedError\n",
    "    def split(self, items):       \n",
    "        \"Explain how so split the `items`. Return two disjoint lists of indices or two boolean masks.\"\n",
    "        raise NotImplementedError\n",
    "    def label(self, items):       \n",
    "        \"Explain how to label your `items`. Return a list of labels.\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __init__(self, tfms=None, tfms_x=None, tfms_y=None, tfms_xy=None):\n",
    "        self.source = self.get_source()\n",
    "        items = ItemList(self.get_items(self.source)) #Just for fancy indexing\n",
    "        split_idx = self.split(items)\n",
    "        labels = ItemList(self.label(items))          #Just for fancy indexing\n",
    "        if tfms_x is None: tfms_x = self.x_cls().init_tfms()\n",
    "        if tfms_y is None: tfms_y = self.y_cls().init_tfms()\n",
    "        x_train,x_valid = map(lambda o: ItemList(items[o],  tfms=tfms_x, item_type=self.x_cls()), split_idx)\n",
    "        y_train,y_valid = map(lambda o: ItemList(labels[o], tfms=tfms_y, item_type=self.y_cls()), split_idx)\n",
    "        if tfms_xy is None: \n",
    "            tfms_xy = self.x_cls().init_tfms(xy=True) + self.y_cls().init_tfms(xy=True)\n",
    "        self.train = LabeledData(x_train, y_train, tfms=listify(tfms) + tfms_xy)\n",
    "        self.valid = LabeledData(x_valid, y_valid, tfms=listify(tfms) + tfms_xy)\n",
    "    \n",
    "    def databunch(self, bs=64, **kwargs):\n",
    "        \"How to convert to a `DataBunch`. Subclass if needed.\"\n",
    "        dls = [get_dl(ds, bs, shuffle=s, drop_last=s, **kwargs) for (ds, s) in zip([self.train, self.valid], [True,False])]\n",
    "        return DataBunch(*dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOpener(Transform):\n",
    "    def __init__(self, **kwargs): super().__init__() #This line can be removed when we make this a ImageTransform\n",
    "    def __call__(self, o): return PIL.Image.open(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Image(Item):\n",
    "    default_tfms = ImageOpener\n",
    "    def __init__(self, cmap=None, alpha=1.): self.cmap,self.alpha = cmap,alpha\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        ax.imshow(x, cmap=self.cmap, alpha=self.alpha)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryEncoder(Transform):\n",
    "    \"Encodes a categorical variable to index.\"\n",
    "    def __init__(self): self.vocab=None\n",
    "    \n",
    "    def setup(self, items):\n",
    "        if self.vocab is not None: return\n",
    "        self.vocab = uniqueify(items, sort=True)\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "    \n",
    "    def __call__(self, o): return self.otoi[o]\n",
    "    def undo(self, i):    return self.vocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Category(Item):\n",
    "    default_tfms = CategoryEncoder\n",
    "    def show(self, x, ax): ax.set_title(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = CategoryEncoder()\n",
    "#Even if 'dog' is the first class, vocab is sorted for reproducibility\n",
    "tfm.setup(['dog', 'cat', 'cat', 'dog', 'cat', 'dog'])\n",
    "test_eq(tfm.vocab,['cat', 'dog'])\n",
    "test_eq([tfm(o) for o in ['dog', 'cat', 'cat']], [1,0,0])\n",
    "test_eq(tfm('cat'),0)\n",
    "test_eq(tfm.undo(1),'dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_image_files(path, include=None):\n",
    "    \"Get image files in `path` recursively.\"\n",
    "    return get_files(path, extensions=image_extensions, recurse=True, include=include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "test_eq(len(get_image_files(path)),1428)\n",
    "test_eq(len(get_image_files(path/'train')),709)\n",
    "test_eq(len(get_image_files(path, include='train')),709)\n",
    "test_eq(len(get_image_files(path, include=['train','valid'])),1408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def random_splitter(items, valid_pct=0.2, seed=None):\n",
    "    \"Split `items` between train/val with `valid_pct` randomly.\"\n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    rand_idx = torch.randperm(len(items))\n",
    "    cut = int(valid_pct * len(items))\n",
    "    return rand_idx[cut:],rand_idx[:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "trn,val = random_splitter([0,1,2,3,4,5], seed=42)\n",
    "test_eq(trn, tensor([3, 2, 4, 1, 5]))\n",
    "test_eq(val, tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _grandparent_mask(items, name):\n",
    "    return [(o.parent.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-2]) == name for o in items]\n",
    "\n",
    "def grandparent_splitter(items, train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the grand parent folder names (`train_name` and `valid_name`).\"\n",
    "    return _grandparent_mask(items, train_name),_grandparent_mask(items, valid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#With string filenames\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = [path/'train'/'3'/'9932.png', path/'valid'/'7'/'7189.png', \n",
    "         path/'valid'/'7'/'7320.png', path/'train'/'7'/'9833.png',  \n",
    "         path/'train'/'3'/'7666.png', path/'valid'/'3'/'925.png',\n",
    "         path/'train'/'7'/'724.png', path/'valid'/'3'/'93055.png']\n",
    "trn,val = grandparent_splitter(items)\n",
    "test_eq(trn,[True,False,False,True,True,False,True,False])\n",
    "test_eq(val,[False,True,True,False,False,True,False,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parent_labeller(items):\n",
    "    \"Label `items` with the parent folder name.\"\n",
    "    return [o.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-1] for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(parent_labeller(items),['3','7','7','7','3','3','7','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def func_labeller(items, func):\n",
    "    \"Label `items` according to `func`.\"\n",
    "    return [func(o) for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(func_labeller(items, lambda x: int(x.parent.name)+1),[4,8,8,8,4,4,8,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def re_labeller(items, pat):\n",
    "    \"Label `items` with a regex `pat`.\"\n",
    "    pat = re.compile(pat)\n",
    "    def _inner(o):\n",
    "        res = pat.search(str(o))\n",
    "        assert res,f'Failed to find \"{pat}\" in \"{o}\"'\n",
    "        return res.group(1)\n",
    "    return func_labeller(items, _inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "pat = re.compile(r'/([^/]+)/\\d+.png$')\n",
    "test_eq(re_labeller(items, pat),['3','7','7','7','3','3','7','3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetsData(DataBlock):\n",
    "    x_cls = Image\n",
    "    y_cls = Category\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.PETS)\n",
    "    def get_items(self, source): return get_image_files(source/\"images\")\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return re_labeller(items, pat = r'/([^/]+)_\\d+.jpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PetsData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "TfmY = Enum('TfmY', 'No Mask Image Point Bbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageTransform(Transform): \n",
    "    \"Basic class for data augmentation transforms.\"\n",
    "    _order=0\n",
    "    _tfm_y_func={TfmY.Image: 'apply_img',   TfmY.Mask: 'apply_mask', TfmY.No: 'noop',\n",
    "                 TfmY.Point: 'apply_point', TfmY.Bbox: 'apply_bbox'}\n",
    "    _undo_y_func={TfmY.Image: 'unapply_img',   TfmY.Mask: 'unapply_mask', TfmY.No: 'noop',\n",
    "                  TfmY.Point: 'unapply_point', TfmY.Bbox: 'unapply_bbox'}\n",
    "    \n",
    "    def __init__(self, tfm_y=TfmY.No): self.tfm_y=tfm_y\n",
    "        \n",
    "    def apply(self, x):       return x\n",
    "    def apply_img(self, y):   return self.apply(y)\n",
    "    def apply_mask(self, y):  return self.apply_img(y)\n",
    "    def apply_point(self, y): return y\n",
    "    def apply_bbox(self, y):  return self.apply_point(y)\n",
    "    \n",
    "    def randomize(self): pass\n",
    "    \n",
    "    def __call__(self, o):\n",
    "        (x,y) = o\n",
    "        self.x = x #Saves the x in case it's needed in the apply for y (x.size for apply_point for instance)\n",
    "        self.randomize() #Ensures we have the same state for x and y\n",
    "        return self.apply(x),getattr(self, self._tfm_y_func[self.tfm_y], noop)(y)\n",
    "    \n",
    "    def unapply(self, x):       return x\n",
    "    def unapply_img(self, y):   return self.unapply(y)\n",
    "    def unapply_mask(self, y):  return self.unapply_img(y)\n",
    "    def unapply_bbox(self, y):  return self.unapply_point(y)\n",
    "    \n",
    "    def undo(self, o):\n",
    "        (x,y) = o\n",
    "        return self.unapply(x),getattr(self, self._undo_y_func[self.tfm_y], noop)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DecodeImg(ImageTransform):\n",
    "    \"Convert regular image to RGB, masks to L mode.\"\n",
    "    def __init__(self, tfm_y=TfmY.No, mode_x='RGB', mode_y=None):\n",
    "        super().__init__(tfm_y)\n",
    "        self.mode_x,self.mode_y = mode_x,mode_y\n",
    "        \n",
    "    def apply(self, x):       return x.convert(self.mode_x)\n",
    "    def apply_image(self, y): return y.convert(self.mode_x if self.mode_y is None else self.mode_y)\n",
    "    def apply_mask(self, y):  return y.convert('L' if self.mode_y is None else self.mode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResizeFixed(ImageTransform):\n",
    "    \"Resize image to `size` using `mode_x` (and `mode_y` on targets).\"\n",
    "    _order=10\n",
    "    def __init__(self, size, tfm_y=TfmY.No, mode_x=PIL.Image.BILINEAR, mode_y=None):\n",
    "        super().__init__(tfm_y)\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        size = (size[1],size[0]) #PIL takes size in the otherway round\n",
    "        self.size,self.mode_x,self.mode_y = size,mode_x,mode_y\n",
    "        \n",
    "    def apply(self, x):       return x.resize(self.size, self.mode_x)\n",
    "    def apply_image(self, y): return y.resize(self.size, self.mode_x if self.mode_y is None else self.mode_y)\n",
    "    def apply_mask(self, y):  return y.resize(self.size, PIL.Image.NEAREST if self.mode_y is None else self.mode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToByteTensor(ImageTransform):\n",
    "    \"Transform our items to byte tensors.\"\n",
    "    _order=20\n",
    "    \n",
    "    def apply(self, x):\n",
    "        res = torch.ByteTensor(torch.ByteStorage.from_buffer(x.tobytes()))\n",
    "        w,h = x.size\n",
    "        return res.view(h,w,-1).permute(2,0,1)\n",
    "    \n",
    "    def unapply(self, x): return x[0] if x.shape[0] == 1 else x.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToFloatTensor(ImageTransform):\n",
    "    \"Transform our items to float tensors (int in the case of mask).\"\n",
    "    _order=20\n",
    "    def __init__(self, tfm_y=TfmY.No, div_x=255., div_y=None):\n",
    "        super().__init__(tfm_y)\n",
    "        self.div_x,self.div_y = div_x,div_y\n",
    "    def apply(self, x):      return x.float().div_(self.div_x)\n",
    "    def apply_mask(self, x): \n",
    "        return x.long() if self.div_y is None else x.long().div_(self.div_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [DecodeImg(), ResizeFixed(128), ToByteTensor(), ToFloatTensor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PetsData(tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(DataBlock):\n",
    "    x_cls = Image\n",
    "    y_cls = Category\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.MNIST)\n",
    "    def get_items(self, source): return get_image_files(source)\n",
    "    def split(self, items):      return grandparent_splitter(items, train_name='training', valid_name='testing')\n",
    "    def label(self, items):      return parent_labeller(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(tfms=[ToByteTensor(), ToFloatTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmap is specified in the `item_get` for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.x.item_type.cmap='gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PLANET_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCategoryEncoder(Transform):\n",
    "    \"Encodes a categorical variable to index.\"\n",
    "    def __init__(self, do_encode=True, classes=None): \n",
    "        assert do_encode or classes is not None, \"If you use one_hot encoded items, please provide classes.\"\n",
    "        self.vocab,self.do_encode=None,do_encode\n",
    "        self.vocab = classes\n",
    "    \n",
    "    def setup(self, items):\n",
    "        if self.vocab is not None: return\n",
    "        vocab = set()\n",
    "        for c in items: vocab = vocab.union(set(c))\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab.sort()\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "    \n",
    "    def __call__(self, item): \n",
    "        if not self.do_encode: return item\n",
    "        return onehot([self.otoi[o] for o in item if o in self.otoi], len(self.vocab))\n",
    "    \n",
    "    def undo(self, o): return [self.vocab[i] for i,v in enumerate(o) if v==1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiCategory(Item):\n",
    "    default_tfms = MultiCategoryEncoder\n",
    "    def show(self, x, ax): ax.set_title(';'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "tfm = MultiCategoryEncoder()\n",
    "#Even if 'c' is the first class, vocab is sorted for reproducibility\n",
    "tfm.setup([['c','a'], ['a','b'], ['b']])\n",
    "test_eq(tfm.vocab,['a','b','c'])\n",
    "\n",
    "test_eq(tfm(['b','a']),tensor([1.,1.,0.]))\n",
    "test_eq(tfm.undo(tensor([1.,0.,1.])),['a','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetData(DataBlock):\n",
    "    x_cls = Image\n",
    "    y_cls = MultiCategory\n",
    "    \n",
    "    def get_source(self):        \n",
    "        self.path = untar_data(URLs.PLANET_SAMPLE)\n",
    "        return pd.read_csv(path/'labels.csv')\n",
    "    def get_items(self, source): return read_column(source, 'image_name', prefix=f'{self.path}/train/', suffix='.jpg')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return read_column(self.source, 'tags', delim=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PlanetData(tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data.train.y.tfms[0].vocab\n",
    "otoi = {s:i for i,s in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetData1(DataBlock):\n",
    "    x_cls = Image\n",
    "    y_cls = MultiCategory\n",
    "    \n",
    "    def get_source(self):        \n",
    "        self.path = untar_data(URLs.PLANET_SAMPLE)\n",
    "        return pd.read_csv(path/'labels.csv')\n",
    "    def get_items(self, source): return read_column(source, 'image_name', prefix=f'{self.path}/train/', suffix='.jpg')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):  \n",
    "        #This is just for the sake of using one-hot encoded labels, but imagine we have a dataset where it's the case.\n",
    "        tags = read_column(self.source, 'tags', delim=' ')\n",
    "        labels = []\n",
    "        for t in tags:\n",
    "            x = torch.zeros(len(classes))\n",
    "            idx = [otoi.get(l,None) for l in t]\n",
    "            idx = [i for i in idx if i is not None]\n",
    "            x[idx] = 1.\n",
    "            labels.append(x)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PlanetData1(tfms=tfms, tfms_y=MultiCategoryEncoder(do_encode=False, classes=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SegmentMask(Image):\n",
    "    \"An `ItemGetter` for segmentation mask targets.\"\n",
    "    tfm_kwargs = {'tfm_y': TfmY.Mask}\n",
    "    def __init__(self, cmap='tab20', alpha=0.5): \n",
    "        super().__init__(cmap=cmap, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamvidData(DataBlock):\n",
    "    x_cls = Image\n",
    "    y_cls = SegmentMask\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.CAMVID_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        path_lbl = self.source/'labels'\n",
    "        codes = np.loadtxt(self.source/'codes.txt', dtype=str)\n",
    "        return func_labeller(items, lambda x: path_lbl/f'{x.stem}_P{x.suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [DecodeImg(tfm_y=TfmY.Mask), ResizeFixed(128, tfm_y=TfmY.Mask), ToByteTensor(tfm_y=TfmY.Mask), \n",
    "        ToFloatTensor(tfm_y=TfmY.Mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CamvidData(tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biwii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PointsGetter(ItemGetter):\n",
    "    \"An `ItemGetter` for points.\"\n",
    "    default_tfm = TfmY.Point\n",
    "    def __init__(self, procs=None, do_scale=True, y_first=False): \n",
    "        super().__init__(procs)\n",
    "        self.do_scale,self.y_first = do_scale,y_first\n",
    "    \n",
    "    def get(self, o):\n",
    "        \"Inner representation of point is scaled from -1 to 1 and y first.\"\n",
    "        if not isinstance(o, torch.Tensor): o = tensor(o)\n",
    "        o = o.view(-1, 2).float()\n",
    "        if not self.y_first: o = o.flip(1)\n",
    "        if self.do_scale and hasattr(self, '_x') and self._x is not None: \n",
    "            sz = tensor(list(self._x.size)).float()\n",
    "            o = o * 2/sz - 1\n",
    "        return o\n",
    "    \n",
    "    def raw(self, o):\n",
    "        \"Put y second and unscale.\"\n",
    "        o = o.flip(1)\n",
    "        if hasattr(self, '_x') and self._x is not None: \n",
    "            sz = tensor([self._x.shape[1:]]).float()\n",
    "            o = (o + 1) * sz/2\n",
    "        return o\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        params = {'s': 10, 'marker': '.', 'c': 'r'}\n",
    "        ax.scatter(x[:, 1], x[:, 0], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "class FakeImg():\n",
    "    def __init__(self, size): self.size = size\n",
    "fake_img = FakeImg((200,120))\n",
    "\n",
    "#Normal usage\n",
    "get = PointsGetter()\n",
    "il = ItemList([[0,0], [120,0], [0,200], [120,200], [60,100]], item_get=get)\n",
    "#Without getting any context, the points aren't scaled because they don't know the picture size.\n",
    "test_eq(il[1], tensor([[0., 120.]]))\n",
    "with AddXContext(il, fake_img):\n",
    "    test_eq(il[1], tensor([[-1., 1.]]))\n",
    "    test_eq(il[4], tensor([[0., 0.]]))\n",
    "    o = il[1]\n",
    "#Test deproc undoes the scaling and switching when providing with right context (need to be the tensor image)\n",
    "with AddXContext(il, torch.zeros(3, 120, 200)):\n",
    "    test_eq(il.deproc(o), tensor([[120.,0.]]))\n",
    "    \n",
    "#Giving scaled points\n",
    "get1 = PointsGetter(do_scale=False)\n",
    "il1 = ItemList([[-1.,-1.], [1.,-1.], [-1.,1.], [1.,1.], [0.,0.]], item_get=get1)\n",
    "with AddXContext(il, fake_img):\n",
    "    for i in range(5): test_eq(il[i], il1[i])\n",
    "with AddXContext(il1, torch.zeros(3, 120, 200)):\n",
    "    for i in range(5): test_eq(il.obj(i), il1.obj(i))\n",
    "        \n",
    "#Giving scaled points with y_first\n",
    "get2 = PointsGetter(do_scale=False, y_first=True)\n",
    "il2 = ItemList([[-1.,-1.], [-1.,1.], [1.,-1.], [1.,1.], [0.,0.]], item_get=get2)\n",
    "#Without getting any context, the points aren't scaled because they don't know the picture size.\n",
    "with AddXContext(il, fake_img):\n",
    "    for i in range(5): test_eq(il[i], il2[i])\n",
    "with AddXContext(il2, torch.zeros(3, 120, 200)):\n",
    "    for i in range(5): test_eq(il.obj(i), il2.obj(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiwiData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = PointsGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.BIWI_SAMPLE)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        fn2ctr = pickle.load(open(self.source/'centers.pkl', 'rb'))\n",
    "        return func_labeller(items, lambda o:fn2ctr[o.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BiwiData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.data import get_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BBoxProcessor(MultiCategoryProcessor):\n",
    "    \"A processor for bounding boxes.\"\n",
    "    def create_vocab(self, items):\n",
    "        super().create_vocab([c[1] for c in items])\n",
    "        self.vocab.insert(0, 'background')\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "\n",
    "    def proc1(self, item):  return item[0],super().proc1(item[1])\n",
    "    def deproc1(self, idx): return idx[0],super().deproc1(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "def _draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def _draw_rect(ax, b, color='white', text=None, text_size=14):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BBoxGetter(PointsGetter):\n",
    "    \"An `ItemGetter` for bounding boxes.\"\n",
    "    default_proc = BBoxProcessor\n",
    "    default_tfm = TfmY.Bbox\n",
    "     \n",
    "    def get(self, o): return super().get(o[0]).view(-1,4),o[1]\n",
    "    def raw(self, o): return super().raw(o[0].view(-1,2)).view(-1,4),o[1]\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        bbox,label = x\n",
    "        for b,l in zip(bbox, label): \n",
    "            if l != 'background': _draw_rect(ax, [b[1],b[0],b[3]-b[1],b[2]-b[0]], text=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def bb_pad_collate(samples, pad_idx=0):\n",
    "    \"Collate function for bounding boxes targets.\"\n",
    "    max_len = max([len(s[1][1]) for s in samples])\n",
    "    bboxes = torch.zeros(len(samples), max_len, 4)\n",
    "    labels = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    imgs = []\n",
    "    for i,s in enumerate(samples):\n",
    "        imgs.append(s[0][None])\n",
    "        bbs, lbls = s[1]\n",
    "        if not (bbs.nelement() == 0):\n",
    "            bboxes[i,-len(lbls):] = bbs\n",
    "            labels[i,-len(lbls):] = tensor(lbls)\n",
    "    return torch.cat(imgs,0), (bboxes,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = BBoxGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.COCO_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'train')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        images, lbl_bbox = get_annotations(self.source/'train.json')\n",
    "        img2bbox = dict(zip(images, lbl_bbox))\n",
    "        return func_labeller(items, lambda o:img2bbox[o.name])\n",
    "    \n",
    "    def databunch(self, bs=64, **kwargs):\n",
    "        kwargs['collate_fn'] = bb_pad_collate\n",
    "        return super().databunch(bs=bs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CocoData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python notebook2script.py \"200_datablock_config.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
